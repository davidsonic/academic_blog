[{"authors":[],"categories":["DRL"],"content":"More details are here \n","date":1573109273,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573109273,"objectID":"c96bdbec943e79e86e630d71d9d3ea5b","permalink":"https://davidsonic.github.io/academic_blog/post/robot-tough-love/","publishdate":"2019-11-07T14:47:53+08:00","relpermalink":"/academic_blog/post/robot-tough-love/","section":"post","summary":"More details are here","tags":["Deep Reinforcement Learning","Robotics"],"title":"USC Media Coverage of Our Work!","type":"post"},{"authors":[],"categories":["C++"],"content":"Powerpoint for File IO in C/C++. Download link: https://github.com/davidsonic/EE155_IO/raw/master/EE155.pptx\n","date":1572418073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572418073,"objectID":"f28cbdcb5673c34534d01894905ec34f","permalink":"https://davidsonic.github.io/academic_blog/post/ee155/","publishdate":"2019-10-30T14:47:53+08:00","relpermalink":"/academic_blog/post/ee155/","section":"post","summary":"Powerpoint for File IO in C/C++. Download link: https://github.com/davidsonic/EE155_IO/raw/master/EE155.pptx","tags":["C++ programming"],"title":"EE155 File IO","type":"post"},{"authors":[],"categories":["DL"],"content":"As TA for USC Deep Learning course (EE599), I was impressed by the representations at the end of this semester from graduate students of USC. More details can be accessed here.\n","date":1556693273,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556693273,"objectID":"1262ae1131081e90f95e2d26c600c022","permalink":"https://davidsonic.github.io/academic_blog/post/deep-learning/","publishdate":"2019-05-01T14:47:53+08:00","relpermalink":"/academic_blog/post/deep-learning/","section":"post","summary":"As TA for USC Deep Learning course (EE599), I was impressed by the representations at the end of this semester from graduate students of USC. More details can be accessed here.","tags":["USC Deep Learning"],"title":"USC Deep Learning Presentation","type":"post"},{"authors":[],"categories":["DL"],"content":"Group seminar report, summarizing progress with regard to \u0026ldquo;LEARNING HUMAN-ROBOT AND ROBOT-WORLD INTERACTION\u0026rdquo;. Online website available at: https://davidsonic.github.io/summary/physical_inference#/.\n","date":1555829273,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555829273,"objectID":"5c52d87881311f3227da5827d9d139ab","permalink":"https://davidsonic.github.io/academic_blog/post/group_seminar_physics/","publishdate":"2019-04-21T14:47:53+08:00","relpermalink":"/academic_blog/post/group_seminar_physics/","section":"post","summary":"Group seminar report, summarizing progress with regard to \u0026ldquo;LEARNING HUMAN-ROBOT AND ROBOT-WORLD INTERACTION\u0026rdquo;. Online website available at: https://davidsonic.github.io/summary/physical_inference#/.","tags":["Generative Adversarial Networks"],"title":"Group Seminar Presentation","type":"post"},{"authors":[],"categories":["DRL"],"content":"Our demo in exhibition for USC Robotics Open House. My colleagues and I presented \u0026ldquo;Robust Grasping via Human Adversarial\u0026rdquo; to visitors and explained the motivation behind the algorithm. Our demo is implemented in customized simulation environment based on physics engine mujoco and supports real-time human interactions.\nDuring the day, we give users the opportunity to apply perturbations to objects via keyboards and mouse, and we show that the manipulator\u0026rsquo;s grasping skill as well as robustness increases over time.\n","date":1555742873,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555742873,"objectID":"8d678dc2936f6ba0368822616a24b9aa","permalink":"https://davidsonic.github.io/academic_blog/post/open-house/","publishdate":"2019-04-20T14:47:53+08:00","relpermalink":"/academic_blog/post/open-house/","section":"post","summary":"Our demo in exhibition for USC Robotics Open House. My colleagues and I presented \u0026ldquo;Robust Grasping via Human Adversarial\u0026rdquo; to visitors and explained the motivation behind the algorithm. Our demo is implemented in customized simulation environment based on physics engine mujoco and supports real-time human interactions.\nDuring the day, we give users the opportunity to apply perturbations to objects via keyboards and mouse, and we show that the manipulator\u0026rsquo;s grasping skill as well as robustness increases over time.","tags":["Deep Reinforcement Learning","Robotics"],"title":"USC Robotics Open House","type":"post"},{"authors":["Jiali Duan","Qian Wang","Lerrel Pinto","C.-C. Jay Kuo","Stefanos Nikolaidis"],"categories":null,"content":"","date":1555737863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555737863,"objectID":"d95ea8be4e2f57516960d8ee079b18c2","permalink":"https://davidsonic.github.io/academic_blog/publication/robot-socal/","publishdate":"2019-04-20T13:24:23+08:00","relpermalink":"/academic_blog/publication/robot-socal/","section":"publication","summary":"To appear at 2019 Caltech Robotics Symposium (http://scr2019.caltech.edu/).","tags":[],"title":"Robust Grasping via Human Adversary","type":"publication"},{"authors":[],"categories":["GAN"],"content":"Presentation slides can be accessed via browser on computer or mobile devices: https://davidsonic.github.io/summary/guest-lecture-gan.html#/\n","date":1551682073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551682073,"objectID":"9b4b5bb2dd6557686675d3d5e0fad5d6","permalink":"https://davidsonic.github.io/academic_blog/post/guest-lecture-ee599/","publishdate":"2019-03-04T14:47:53+08:00","relpermalink":"/academic_blog/post/guest-lecture-ee599/","section":"post","summary":"Presentation slides can be accessed via browser on computer or mobile devices: https://davidsonic.github.io/summary/guest-lecture-gan.html#/","tags":["Generative Adversarial Networks"],"title":"Guest Lecture for EE599 (USC Deep Learning Course)","type":"post"},{"authors":["Jiali Duan","Qian Wang","Lerrel Pinto","C.-C. Jay Kuo","Stefanos Nikolaidis"],"categories":null,"content":"","date":1551677063,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551677063,"objectID":"a19f0faa93f429bf5724eae026e9981d","permalink":"https://davidsonic.github.io/academic_blog/publication/robot-iros/","publishdate":"2019-03-04T13:24:23+08:00","relpermalink":"/academic_blog/publication/robot-iros/","section":"publication","summary":"Much work in robotics has focused on “human- in-the-loop” learning techniques that improve the efficiency of the learning process. However, these algorithms have made the strong assumption of a cooperating human supervisor that assists the robot. In reality, human observers tend to also act in an adversarial manner towards deployed robotic systems. We show that this can in fact improve the robustness of the learned models by proposing a physical framework that leverages perturbations applied by a human adversary, guiding the robot towards more robust models. In a manipulation task, we show that grasping success improves significantly when the robot trains with a human adversary as compared to training in a self-supervised manner.","tags":[],"title":"Robot Learning via Human Adversarial Games","type":"publication"},{"authors":[],"categories":["DRL"],"content":"In the context of reinforcement learning, Mujoco + gym is more popular than Gazebo in research that involve robotics. However, mujoco-py released by OpenAI doesn\u0026rsquo;t provide full flexibility compared to original Mujoco C++ API. In a recent work of mine, I upgraded mujoco-py==1.5.0 that supports:\n Interactive manipulation as provided by simulate in Mujoco, written in Cython\n Force visualization similar to deepmind-control but allows for headless rendering\n  The code is available at https://github.com/davidsonic/self_brewed_mujoco_py\nBased on self-brewed-mujoco-py, inverse kinmatics for Baxter robot was implemented, which given a snapshot of current scene, the robotic arm could automatically locate and perform grasping.\nThis is different with PickAndPlace task in latest gym environment, which requires reinforced PPO with millions of frames. My goal is to learn an adversary that would in turn assist robust robotic arm grasping.\nA result of this procedure is available at https://davidsonic.github.io/summary/videos/inverse-kinmatics.mp4\n","date":1547794073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547794073,"objectID":"84aaa00f4c152938b980396a5e63b33a","permalink":"https://davidsonic.github.io/academic_blog/post/robust_mainpulator/","publishdate":"2019-01-18T14:47:53+08:00","relpermalink":"/academic_blog/post/robust_mainpulator/","section":"post","summary":"In the context of reinforcement learning, Mujoco + gym is more popular than Gazebo in research that involve robotics. However, mujoco-py released by OpenAI doesn\u0026rsquo;t provide full flexibility compared to original Mujoco C++ API. In a recent work of mine, I upgraded mujoco-py==1.5.0 that supports:\n Interactive manipulation as provided by simulate in Mujoco, written in Cython\n Force visualization similar to deepmind-control but allows for headless rendering\n  The code is available at https://github.","tags":["Deep Reinforcement Learning"],"title":"Robust Grasping with Adversary","type":"post"},{"authors":[],"categories":["DRL"],"content":"Deterministic policy gradient is a variation of A2C, but is off-policy. In A2C, the actor estimates the stochastic policy, either in the form of probability distribute over discrete actions or, the parameters fo normal distribution. DPG also belong to the A2C family, but its policy is deterministic. This makes it possible to apply the chain rule to maximize the Q-value.\nDPG has to components. First is the actor. In a continuous action domain, every action is a number, so the actor network will take the state as input and output N values, one for each action. This mapping is deterministic. Second is the critic, which is used to estimate the Q-value, calculated as the discounted reward of the action taken at some state Q(s,a). Its inputs include state and action (a vector) and outputs a single number corresponding to the Q-value.\nNow, we can substitute the actor function into the critic and get Q(s, u(s)), which we are interested to approximate in the first place. It depends not only on state, but also the parameters of actor and critic network. At every step of optimization, we want to change the actor\u0026rsquo;s weight to improve the total reward. Expressed in mathematics, we want to get the gradient of the policy: $\\triangledown_{a}Q(s,a)_{|a=\\mu(s)}\\triangledown_{\\theta_{\\mu}}\\mu(s) $.\nNote that despite both A2C and DDPG belonging to the A2C family, critic is used in different ways. In A2C, critic is used as a baseline for calculating advantage for improving stability. In DDPG, as our policy is deterministic, we can calculate the gradient from Q, obtained from critic up to actor\u0026rsquo;s weights, so the whole system is end-to-end differentiable with SGD. To update the critic network, Bellman equation is used to approximate Q(s,a) and MSE objective is minimized.\nThe idea of DDPG is quite intuitive, the critic is updated as in A2C and the actor is updated in a way to maximize the critic\u0026rsquo;s output. Additionally, it\u0026rsquo;s off-policy, which leverages a replay buffer to improve sample efficiency just like DQN.\nA result trained with DDPG is at https://davidsonic.github.io/summary/videos/grasp_DDPG.mp4\n","date":1547362073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547362073,"objectID":"e4324e88b77ff68a7c44b1e1b3f033d8","permalink":"https://davidsonic.github.io/academic_blog/post/ddpg/","publishdate":"2019-01-13T14:47:53+08:00","relpermalink":"/academic_blog/post/ddpg/","section":"post","summary":"Deterministic policy gradient is a variation of A2C, but is off-policy. In A2C, the actor estimates the stochastic policy, either in the form of probability distribute over discrete actions or, the parameters fo normal distribution. DPG also belong to the A2C family, but its policy is deterministic. This makes it possible to apply the chain rule to maximize the Q-value.\nDPG has to components. First is the actor. In a continuous action domain, every action is a number, so the actor network will take the state as input and output N values, one for each action.","tags":["Deep Reinforcement Learning"],"title":"Review DDPG","type":"post"},{"authors":[],"categories":["DRL"],"content":"Poster for this research is available at https://davidsonic.github.io/summary/Poster_3d_indoor.pdf\nNews about the project is http://mcl.usc.edu/news/2018/12/02/mcl-research-on-re-enforcement-learning/\nTrajectory visualization is at http://mcl-lab.usc.edu:3000/trajectory.html\nBrief report for this project is https://davidsonic.github.io/summary/EE546Final-JialiDuan.pdf\n","date":1541227673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541227673,"objectID":"fb3c2ff21b3f0fd5ffa0a7b61f59e50a","permalink":"https://davidsonic.github.io/academic_blog/post/3d-indoor/","publishdate":"2018-11-03T14:47:53+08:00","relpermalink":"/academic_blog/post/3d-indoor/","section":"post","summary":"Poster for this research is available at https://davidsonic.github.io/summary/Poster_3d_indoor.pdf\nNews about the project is http://mcl.usc.edu/news/2018/12/02/mcl-research-on-re-enforcement-learning/\nTrajectory visualization is at http://mcl-lab.usc.edu:3000/trajectory.html\nBrief report for this project is https://davidsonic.github.io/summary/EE546Final-JialiDuan.pdf","tags":["Deep Reinforcement Learning"],"title":"Language guided visual 3D indoor navigation","type":"post"},{"authors":null,"categories":null,"content":"","date":1540371801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540371801,"objectID":"a29a35b64e847505b6e13d15fbd9e687","permalink":"https://davidsonic.github.io/academic_blog/project/rl-indoor-navigation/","publishdate":"2018-10-24T17:03:21+08:00","relpermalink":"/academic_blog/project/rl-indoor-navigation/","section":"project","summary":"3D indoor navigation using reinforcement learning. Webgl rendered visualization engine.","tags":[],"title":"RL 3D Indoor Navigation","type":"project"},{"authors":["Yao Zhu","Saksham Suri","Pranav Kulkarni","Yueru Chen","Jiali Duan","C.-C. Jay Kuo"],"categories":null,"content":"","date":1540358663,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540358663,"objectID":"229af7b38a4fb7733279817c16bdbf67","permalink":"https://davidsonic.github.io/academic_blog/publication/digit-syn/","publishdate":"2018-10-24T13:24:23+08:00","relpermalink":"/academic_blog/publication/digit-syn/","section":"publication","summary":"An interpretable generative model for handwritten digits synthe- sis is proposed in this work. Modern image generative models, such as Generative Adversarial Networks (GANs) and Variational Au- toencoders (VAEs), are trained by backpropagation (BP). The train- ing process is complex and the underlying mechanism is difficult to explain. We propose an interpretable multi-stage PCA method to achieve the same goal and use handwritten digit images synthesis as an illustrative example. First, we derive principal-component- analysis-based (PCA-based) transform kernels at each stage based on the covariance of its inputs. This results in a sequence of trans- forms that convert input images of correlated pixels to spectral vec- tors of uncorrelated components. In other words, it is a whitening process. Then, we can synthesize an image based on random vectors and multi-stage transform kernels through a coloring process. The generative model is a feedforward (FF) design since no BP is used in model parameter determination. Its design complexity is signifi- cantly lower, and the whole design process is explainable. Finally, we design an FF generative model using the MNIST dataset, com- pare synthesis results with those obtained by state-of-the-art GAN and VAE methods, and show that the proposed generative model achieves comparable performance.","tags":[],"title":"AN INTERPRETABLE GENERATIVE MODEL FOR HANDWRITTEN DIGIT IMAGE SYNTHESIS","type":"publication"},{"authors":[],"categories":["DRL"],"content":"This group seminar presents an introduction to RL and its interesting applications. See my slide at \u0026ldquo;https://davidsonic.github.io/summary/#/\u0026quot;.\nA more fundamental intro is at https://davidsonic.github.io/summary/index_bak_bak.html#/ .\n","date":1539154073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539154073,"objectID":"00719efc460164af8408efbb27e5bbe3","permalink":"https://davidsonic.github.io/academic_blog/post/drl/","publishdate":"2018-10-10T14:47:53+08:00","relpermalink":"/academic_blog/post/drl/","section":"post","summary":"This group seminar presents an introduction to RL and its interesting applications. See my slide at \u0026ldquo;https://davidsonic.github.io/summary/#/\u0026quot;.\nA more fundamental intro is at https://davidsonic.github.io/summary/index_bak_bak.html#/ .","tags":["Deep Reinforcement Learning"],"title":"Group Seminar Presentation on RL introduction","type":"post"},{"authors":["C.C-Jay Kuo","Min Zhang","Siyang Li","Jiali Duan","Yueru Chen"],"categories":null,"content":"","date":1537766663,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537766663,"objectID":"6b60f13e6f3ac9e8ed5fd82328cbfabd","permalink":"https://davidsonic.github.io/academic_blog/publication/interpretable/","publishdate":"2018-09-24T13:24:23+08:00","relpermalink":"/academic_blog/publication/interpretable/","section":"publication","summary":"The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable feedforward (FF) design without any BP as a reference. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace Approximation with Adjusted Bias) transform. It is a variant of the principal component analysis (PCA) with an added bias vector to annihilate activation's nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected (FC) layers, we construct them using a cascade of multi-stage linear least squared regressors (LSRs). The classification and robustness (against adversarial attacks) performances of BP- and FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.","tags":[],"title":"Interpretable Convolutional Neural Networks via Feedforward Design","type":"publication"},{"authors":null,"categories":null,"content":"","date":1535101401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535101401,"objectID":"9c40632dbdeea5f0b5ecc8af279252d2","permalink":"https://davidsonic.github.io/academic_blog/project/portraitgan/","publishdate":"2018-08-24T17:03:21+08:00","relpermalink":"/academic_blog/project/portraitgan/","section":"project","summary":"PortraitGAN enables interactive continuous editing of human portraits in resolution of 512x512. More details are in the paper PortraitGAN for Flexible Portrait Manipulation","tags":[],"title":"Portrait Manipulation with GAN (offline)","type":"project"},{"authors":null,"categories":null,"content":"","date":1535100912,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535100912,"objectID":"396b5f86bf2f3a97783947cba17994be","permalink":"https://davidsonic.github.io/academic_blog/project/unity-webgl/","publishdate":"2018-08-24T16:55:12+08:00","relpermalink":"/academic_blog/project/unity-webgl/","section":"project","summary":"Unity-webgl game programming project. Use 'w' to thrust, 'a' and 'd' for left and right respectively.","tags":[],"title":"Unity Webgl (Try)","type":"project"},{"authors":null,"categories":null,"content":"","date":1535100374,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535100374,"objectID":"03685147f10f8bf7f25440e2e63997b6","permalink":"https://davidsonic.github.io/academic_blog/project/mcl-server/","publishdate":"2018-08-24T16:46:14+08:00","relpermalink":"/academic_blog/project/mcl-server/","section":"project","summary":"Maintain and extend server scheduling system for MCL lab","tags":[],"title":"Mcl Server","type":"project"},{"authors":null,"categories":null,"content":"","date":1535100110,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535100110,"objectID":"6984f70816264e272ad2f62e62de63d1","permalink":"https://davidsonic.github.io/academic_blog/project/saak-transform/","publishdate":"2018-08-24T16:41:50+08:00","relpermalink":"/academic_blog/project/saak-transform/","section":"project","summary":"Reimplement On Data-Driven Saak Transform with pytorch","tags":[],"title":"Saak Transform","type":"project"},{"authors":null,"categories":null,"content":"","date":1535099886,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535099886,"objectID":"d89fcc5d3ba365fba980a02a7366817c","permalink":"https://davidsonic.github.io/academic_blog/project/opengl-graphic/","publishdate":"2018-08-24T16:38:06+08:00","relpermalink":"/academic_blog/project/opengl-graphic/","section":"project","summary":"Implement computer-graphics algorithms with latest opengl features","tags":[],"title":"Opengl Graphic","type":"project"},{"authors":null,"categories":null,"content":"","date":1535099662,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535099662,"objectID":"3234ec4a123f41aba764dae0cae3b2d8","permalink":"https://davidsonic.github.io/academic_blog/project/mit-lids/","publishdate":"2018-08-24T16:34:22+08:00","relpermalink":"/academic_blog/project/mit-lids/","section":"project","summary":"Alumni demonstration website for MIT LIDS with complete front-end and back-end features included","tags":[],"title":"Mit-lids Alumni website","type":"project"},{"authors":["Jiali Duan","Xiaoyuan Guo","Yuhang Song","Chao Yang","C.C-Jay Kuo"],"categories":null,"content":"","date":1535088263,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535088263,"objectID":"cd9e879b8040fb7031afd8c8491c801b","permalink":"https://davidsonic.github.io/academic_blog/publication/portraitgan/","publishdate":"2018-08-24T13:24:23+08:00","relpermalink":"/academic_blog/publication/portraitgan/","section":"publication","summary":"Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.","tags":[],"title":"PortraitGAN for Flexible Portrait Manipulation","type":"publication"},{"authors":["Jiali Duan"],"categories":["full-stack"],"content":"Full-featured MIT-LIDS ALUM website is now deployed on Amazon webservice at: http://www.lids-alum.org . It now serves as the portal site for MIT ALUMs.\n","date":1523342873,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523342873,"objectID":"a13eee508aa1e95ea574baef5074083b","permalink":"https://davidsonic.github.io/academic_blog/post/mit-lids/","publishdate":"2018-04-10T14:47:53+08:00","relpermalink":"/academic_blog/post/mit-lids/","section":"post","summary":"Full-featured MIT-LIDS ALUM website is now deployed on Amazon webservice at: http://www.lids-alum.org . It now serves as the portal site for MIT ALUMs.","tags":["MIT-LIDS"],"title":"MIT-LIDS ALUM website now online","type":"post"},{"authors":null,"categories":null,"content":"","date":1503557185,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503557185,"objectID":"e8fe18a2c2bb98ea8066148858bd69ca","permalink":"https://davidsonic.github.io/academic_blog/project/prior2017/","publishdate":"2017-08-24T14:46:25+08:00","relpermalink":"/academic_blog/project/prior2017/","section":"project","summary":"Projects collaborated with professors and students at Institute of Automation, Chinese Academy of Sciences","tags":[],"title":"Prior 2017 projects","type":"project"},{"authors":["Jiali Duan","Jun Wan","Shuai Zhou","Xiaoyuan Guo","Stan Z. Li"],"categories":null,"content":"","date":1503552794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503552794,"objectID":"89c6b69847bf3b09e173ecf4311320d5","permalink":"https://davidsonic.github.io/academic_blog/publication/multi-model/","publishdate":"2017-08-24T13:33:14+08:00","relpermalink":"/academic_blog/publication/multi-model/","section":"publication","summary":"In this paper, we focus on isolated gesture recognition and explore different modalities by involving RGB stream, depth stream and saliency stream for inspection. Our goal is to push the boundary of this realm even further by proposing a unified framework which exploits the advantages of multi-modality fusion. Specifically, a spatial-temporal network architecture based on consensus-voting has been pro- posed to explicitly model the long term structure of the video sequence and to reduce estimation vari- ance when confronted with comprehensive inter-class variations. In addition, a 3D depth-saliency con- volutional network is aggregated in parallel to capture subtle motion characteristics.","tags":[],"title":"A Unified Framework for Multi-Modal Isolated Gesture Recognition","type":"publication"},{"authors":null,"categories":null,"content":"","date":1472023196,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472023196,"objectID":"108cc31314e5578ca9651879fd53cfbb","permalink":"https://davidsonic.github.io/academic_blog/project/review2016/","publishdate":"2016-08-24T15:19:56+08:00","relpermalink":"/academic_blog/project/review2016/","section":"project","summary":"A demonstration website made for summarization report of year 2016","tags":[],"title":"Review 2016","type":"project"},{"authors":["Jiali Duan","Shuai Zhou","Jun Wan","Xiaoyuan Guo","Stan Z. Li"],"categories":null,"content":"","date":1472017015,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472017015,"objectID":"8584e93b7ea6e0f98a1e7d4ab24e2503","permalink":"https://davidsonic.github.io/academic_blog/publication/gesture-recog/","publishdate":"2016-08-24T13:36:55+08:00","relpermalink":"/academic_blog/publication/gesture-recog/","section":"publication","summary":"We propose a convolutional two-stream consensus voting network (2SCVN) which explicitly models both the short-term and long-term structure of the RGB sequences. To alleviate distractions from background, a 3d depth-saliency ConvNet stream (3DDSN) is aggregated in parallel to identify subtle motion characteristics. These two components in an unified framework significantly improve the recognition accuracy. On the challenging Chalearn IsoGD benchmark, our proposed method outperforms the first place on the leader-board by a large margin (10.29%) while also achieving the best result on RGBD-HuDaAct dataset (96.74%).","tags":[],"title":"Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for Isolated Gesture Recognition","type":"publication"},{"authors":["Jiali Duan","Shengcai Liao","Xiaoyuan Guo","Stan Z. Li"],"categories":null,"content":"","date":1469338889,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469338889,"objectID":"a9814281b378cbb17ee138193276f1da","permalink":"https://davidsonic.github.io/academic_blog/publication/face-detect/","publishdate":"2016-07-24T13:41:29+08:00","relpermalink":"/academic_blog/publication/face-detect/","section":"publication","summary":"In this paper, we propose a novel face detection method called Aggregating Visible Components (AVC), which addresses pose variations and occlusions simultaneously in a single framework with low complexi- ty. The main contributions of this paper are: (1) By aggregating visible components which have inherent advantages in occasions of occlusions, the proposed method achieves state-of-the-art performance using only hand-crafted feature; (2) Mapped from meanshape through component- invariant mapping, the proposed component detector is more robust to pose-variations (3) A local to global aggregation strategy that involves region competition helps alleviate false alarms while enhancing localiza- tion accuracy.","tags":[],"title":"Face Detection by Aggregating Visible Components","type":"publication"},{"authors":["Jiali Duan","Shengcai Liao","Shuai Zhou","Stan Z. Li"],"categories":null,"content":"","date":1464068731,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464068731,"objectID":"bd84ced25f92b2fb610ae0f1687a275d","permalink":"https://davidsonic.github.io/academic_blog/publication/face-classify/","publishdate":"2016-05-24T13:45:31+08:00","relpermalink":"/academic_blog/publication/face-classify/","section":"publication","summary":"We conduct a specialized benchmark study in this paper, which focuses on face classifica tion. We start with face proposals, and build a benchmark dataset with about 3.5 million patches for two-class face/non-face classification. Results with several baseline algorithms show that, without the help of post-processing, the performance of face classification itself is still not very satisfactory, even with a powerful CNN method. We’ll release this benchmark to help assess performance of face classification only, and ease the participation of other related researchers.","tags":[],"title":"Face Classification: A Specialized Benchmark Study","type":"publication"}]