<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.47.1" />
  <meta name="author" content="Jiali Duan">

  
  
  
  
    
  
  <meta name="description" content="Deterministic policy gradient is a variation of A2C, but is off-policy. In A2C, the actor estimates the stochastic policy, either in the form of probability distribute over discrete actions or, the parameters fo normal distribution. DPG also belong to the A2C family, but its policy is deterministic. This makes it possible to apply the chain rule to maximize the Q-value.
DPG has to components. First is the actor. In a continuous action domain, every action is a number, so the actor network will take the state as input and output N values, one for each action.">

  
  <link rel="alternate" hreflang="en-us" href="https://davidsonic.github.io/academic_blog/post/ddpg/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/academic_blog/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://davidsonic.github.io/academic_blog/index.xml" type="application/rss+xml" title="Jiali Duan&#39;s Personal Website">
  <link rel="feed" href="https://davidsonic.github.io/academic_blog/index.xml" type="application/rss+xml" title="Jiali Duan&#39;s Personal Website">
  

  <link rel="manifest" href="/academic_blog/site.webmanifest">
  <link rel="icon" type="image/png" href="/academic_blog/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/academic_blog/img/icon-192.png">

  <link rel="canonical" href="https://davidsonic.github.io/academic_blog/post/ddpg/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Jiali Duan&#39;s Personal Website">
  <meta property="og:url" content="https://davidsonic.github.io/academic_blog/post/ddpg/">
  <meta property="og:title" content="Review DDPG | Jiali Duan&#39;s Personal Website">
  <meta property="og:description" content="Deterministic policy gradient is a variation of A2C, but is off-policy. In A2C, the actor estimates the stochastic policy, either in the form of probability distribute over discrete actions or, the parameters fo normal distribution. DPG also belong to the A2C family, but its policy is deterministic. This makes it possible to apply the chain rule to maximize the Q-value.
DPG has to components. First is the actor. In a continuous action domain, every action is a number, so the actor network will take the state as input and output N values, one for each action.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-01-13T14:47:53&#43;08:00">
  
  <meta property="article:modified_time" content="2019-01-13T14:47:53&#43;08:00">
  

  

  

  <title>Review DDPG | Jiali Duan&#39;s Personal Website</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/academic_blog/">Jiali Duan&#39;s Personal Website</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/academic_blog/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/academic_blog/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/academic_blog/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/academic_blog/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/academic_blog/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Review DDPG</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jiali Duan">
  </span>
  

  <span class="article-date">
    
    <meta content="2019-01-13 14:47:53 &#43;0800 &#43;0800" itemprop="datePublished">
    <time datetime="2019-01-13 14:47:53 &#43;0800 &#43;0800" itemprop="dateModified">
      Jan 13, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jiali Duan">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    2 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://davidsonic.github.io/academic_blog/categories/drl/">DRL</a>
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Review%20DDPG&amp;url=https%3a%2f%2fdavidsonic.github.io%2facademic_blog%2fpost%2fddpg%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdavidsonic.github.io%2facademic_blog%2fpost%2fddpg%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdavidsonic.github.io%2facademic_blog%2fpost%2fddpg%2f&amp;title=Review%20DDPG"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fdavidsonic.github.io%2facademic_blog%2fpost%2fddpg%2f&amp;title=Review%20DDPG"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Review%20DDPG&amp;body=https%3a%2f%2fdavidsonic.github.io%2facademic_blog%2fpost%2fddpg%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>Deterministic policy gradient is a variation of A2C, but is off-policy. In A2C, the actor
estimates the stochastic policy, either in the form of probability distribute over discrete actions or,
the parameters fo normal distribution. DPG also belong to the A2C family, but its policy is deterministic. This
makes it possible to apply the chain rule to maximize the Q-value.</p>

<p>DPG has to components. First is the actor. In a continuous action domain, every action is a number, so the actor
network will take the state as input and output N values, one for each action. This mapping is deterministic.
Second is the critic, which is used to estimate the Q-value, calculated as the discounted reward of the action taken at some
state Q(s,a). Its inputs include state and action (a vector) and outputs a single number corresponding to the Q-value.</p>

<p>Now, we can substitute the actor function into the critic and get Q(s, u(s)), which we are interested to approximate in the first place.
It depends not only on state, but also the parameters of actor and critic network. At every step of optimization, we want to
change the actor&rsquo;s weight to improve the total reward. Expressed in mathematics, we want to get the gradient of the policy: $\triangledown_{a}Q(s,a)_{|a=\mu(s)}\triangledown_{\theta_{\mu}}\mu(s) $.</p>

<p>Note that despite both A2C and DDPG belonging to the A2C family, critic is used in different ways. In A2C, critic is used as a baseline for calculating advantage for
improving stability. In DDPG, as our policy is deterministic, we can calculate the gradient from Q, obtained from critic up to actor&rsquo;s weights, so the whole system
is end-to-end differentiable with SGD. To update the critic network, Bellman equation is used to approximate Q(s,a) and MSE objective is minimized.</p>

<p>The idea of DDPG is quite intuitive, the critic is updated as in A2C and the actor is updated in a way to maximize the critic&rsquo;s output. Additionally, it&rsquo;s off-policy, which
leverages a replay buffer to improve sample efficiency just like DQN.</p>

<p>A result trained with DDPG is at <a href="https://davidsonic.github.io/summary/videos/grasp_DDPG.mp4" target="_blank">https://davidsonic.github.io/summary/videos/grasp_DDPG.mp4</a></p>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://davidsonic.github.io/academic_blog/tags/deep-reinforcement-learning/">Deep Reinforcement Learning</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/academic_blog/post/3d-indoor/">Language guided visual 3D indoor navigation</a></li>
        
        <li><a href="/academic_blog/post/drl/">Group Seminar Presentation on RL introduction</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> and
      <a href="https://www.algolia.com/" target="_blank">Algolia</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/academic_blog/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/academic_blog/search.json";
      const i18n = {
        'placeholder': "Search...",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/academic_blog/js/search.js"></script>
    

    
    

  </body>
</html>

