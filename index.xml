<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiali Duan&#39;s Personal Website on Jiali Duan&#39;s Personal Website</title>
    <link>https://davidsonic.github.io/academic_blog/</link>
    <description>Recent content in Jiali Duan&#39;s Personal Website on Jiali Duan&#39;s Personal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/academic_blog/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Chair for USC Deep Learning Course Presentation</title>
      <link>https://davidsonic.github.io/academic_blog/post/deeplearn-chair/</link>
      <pubDate>Wed, 06 May 2020 17:00:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/deeplearn-chair/</guid>
      <description>&lt;p&gt;Serve as chair for Computer Vision Session of the USC Deep Learning Course 2020 Spring.
More details can be accessed &lt;a href=&#34;https://deeplearning.usc-ece.com/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Guest Lecture for EE599-2020 (USC Deep Learning Course)</title>
      <link>https://davidsonic.github.io/academic_blog/post/guest-lec-ee599-2020/</link>
      <pubDate>Wed, 08 Apr 2020 17:00:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/guest-lec-ee599-2020/</guid>
      <description>&lt;p&gt;Presentation slides can be accessed via browser on computer or mobile devices:  &lt;a href=&#34;https://davidsonic.github.io/summary/rl-guest-EE599#/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Guest Lecture for CSCI699-2020 Computational Human-Robot Interaction</title>
      <link>https://davidsonic.github.io/academic_blog/post/guest-lec-csci699-2020/</link>
      <pubDate>Wed, 01 Apr 2020 15:30:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/guest-lec-csci699-2020/</guid>
      <description>&lt;p&gt;Presentation slides can be accessed via browser on computer or mobile devices &lt;a href=&#34;https://davidsonic.github.io/summary/guest-lecture-reinforce.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>USC Media Coverage of Our Work!</title>
      <link>https://davidsonic.github.io/academic_blog/post/robot-tough-love/</link>
      <pubDate>Thu, 07 Nov 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/robot-tough-love/</guid>
      <description>&lt;p&gt;More details are &lt;a href=&#34;https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/&#34;&gt; here &lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EE155 File IO</title>
      <link>https://davidsonic.github.io/academic_blog/post/ee155/</link>
      <pubDate>Wed, 30 Oct 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/ee155/</guid>
      <description>&lt;p&gt;Powerpoint for File IO in C/C++. Download link is &lt;a href=&#34;https://github.com/davidsonic/EE155_IO/raw/master/EE155.pptx&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>USC Deep Learning Presentation</title>
      <link>https://davidsonic.github.io/academic_blog/post/deep-learning/</link>
      <pubDate>Wed, 01 May 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/deep-learning/</guid>
      <description>&lt;p&gt;As TA for USC Deep Learning course (EE599), I was impressed by the representations at the end of this semester from graduate students of USC.
More details can be accessed &lt;a href=&#34;https://deeplearning.usc-ece.com/index-2019.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Group Seminar Presentation</title>
      <link>https://davidsonic.github.io/academic_blog/post/group_seminar_physics/</link>
      <pubDate>Sun, 21 Apr 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/group_seminar_physics/</guid>
      <description>&lt;p&gt;Group seminar report, summarizing progress with regard to &amp;ldquo;LEARNING HUMAN-ROBOT AND ROBOT-WORLD INTERACTION&amp;rdquo;. Online website available &lt;a href=&#34;https://davidsonic.github.io/summary/physical_inference#/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>USC Robotics Open House</title>
      <link>https://davidsonic.github.io/academic_blog/post/open-house/</link>
      <pubDate>Sat, 20 Apr 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/open-house/</guid>
      <description>&lt;p&gt;Our demo in exhibition for USC Robotics Open House. My colleagues and I presented &amp;ldquo;Robust Grasping via Human Adversarial&amp;rdquo; to  visitors and
explained the motivation behind the algorithm. Our demo is implemented in customized simulation environment based on physics engine mujoco and
supports real-time human interactions.&lt;/p&gt;

&lt;p&gt;During the day, we give users the opportunity to apply perturbations to objects via keyboards and mouse, and we show that the manipulator&amp;rsquo;s grasping skill as
well as robustness increases over time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://davidsonic.github.io/summary/images/kevin.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://davidsonic.github.io/summary/images/pojen.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust Grasping via Human Adversary</title>
      <link>https://davidsonic.github.io/academic_blog/publication/robot-socal/</link>
      <pubDate>Sat, 20 Apr 2019 13:24:23 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/publication/robot-socal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Guest Lecture for EE599 (USC Deep Learning Course)</title>
      <link>https://davidsonic.github.io/academic_blog/post/guest-lecture-ee599/</link>
      <pubDate>Mon, 04 Mar 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/guest-lecture-ee599/</guid>
      <description>&lt;p&gt;Presentation slides can be accessed via browser on computer or mobile devices: &lt;a href=&#34;https://davidsonic.github.io/summary/guest-lecture-gan.html#/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robot Learning via Human Adversarial Games</title>
      <link>https://davidsonic.github.io/academic_blog/publication/robot-iros/</link>
      <pubDate>Mon, 04 Mar 2019 13:24:23 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/publication/robot-iros/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Grasping with Adversary</title>
      <link>https://davidsonic.github.io/academic_blog/post/robust_mainpulator/</link>
      <pubDate>Fri, 18 Jan 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/robust_mainpulator/</guid>
      <description>&lt;p&gt;In the context of reinforcement learning, Mujoco + gym is more popular than Gazebo in research that involve robotics.
However, mujoco-py released by OpenAI doesn&amp;rsquo;t provide full flexibility compared to original Mujoco C++ API.
In a recent work of mine, I upgraded mujoco-py==1.5.0 that supports:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Interactive manipulation as provided by simulate in Mujoco, written in Cython&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Force visualization similar to deepmind-control but allows for headless rendering&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The code is available at &lt;a href=&#34;https://github.com/davidsonic/self_brewed_mujoco_py&#34; target=&#34;_blank&#34;&gt;https://github.com/davidsonic/self_brewed_mujoco_py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Based on self-brewed-mujoco-py, inverse kinmatics for Baxter robot was implemented, which given a snapshot
of current scene, the robotic arm could automatically locate and perform grasping.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://davidsonic.github.io/summary/images/grasp.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is different with
PickAndPlace task in latest gym environment, which requires reinforced PPO with millions of frames. My goal is
to learn an adversary that would in turn assist robust robotic arm grasping.&lt;/p&gt;

&lt;p&gt;A result of this procedure is available &lt;a href=&#34;https://davidsonic.github.io/summary/videos/inverse-kinmatics.mp4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Review DDPG</title>
      <link>https://davidsonic.github.io/academic_blog/post/ddpg/</link>
      <pubDate>Sun, 13 Jan 2019 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/ddpg/</guid>
      <description>&lt;p&gt;Deterministic policy gradient is a variation of A2C, but is off-policy. In A2C, the actor
estimates the stochastic policy, either in the form of probability distribute over discrete actions or,
the parameters fo normal distribution. DPG also belong to the A2C family, but its policy is deterministic. This
makes it possible to apply the chain rule to maximize the Q-value.&lt;/p&gt;

&lt;p&gt;DPG has to components. First is the actor. In a continuous action domain, every action is a number, so the actor
network will take the state as input and output N values, one for each action. This mapping is deterministic.
Second is the critic, which is used to estimate the Q-value, calculated as the discounted reward of the action taken at some
state Q(s,a). Its inputs include state and action (a vector) and outputs a single number corresponding to the Q-value.&lt;/p&gt;

&lt;p&gt;Now, we can substitute the actor function into the critic and get Q(s, u(s)), which we are interested to approximate in the first place.
It depends not only on state, but also the parameters of actor and critic network. At every step of optimization, we want to
change the actor&amp;rsquo;s weight to improve the total reward. Expressed in mathematics, we want to get the gradient of the policy: $\triangledown_{a}Q(s,a)_{|a=\mu(s)}\triangledown_{\theta_{\mu}}\mu(s) $.&lt;/p&gt;

&lt;p&gt;Note that despite both A2C and DDPG belonging to the A2C family, critic is used in different ways. In A2C, critic is used as a baseline for calculating advantage for
improving stability. In DDPG, as our policy is deterministic, we can calculate the gradient from Q, obtained from critic up to actor&amp;rsquo;s weights, so the whole system
is end-to-end differentiable with SGD. To update the critic network, Bellman equation is used to approximate Q(s,a) and MSE objective is minimized.&lt;/p&gt;

&lt;p&gt;The idea of DDPG is quite intuitive, the critic is updated as in A2C and the actor is updated in a way to maximize the critic&amp;rsquo;s output. Additionally, it&amp;rsquo;s off-policy, which
leverages a replay buffer to improve sample efficiency just like DQN.&lt;/p&gt;

&lt;p&gt;A result trained with DDPG is at &lt;a href=&#34;https://davidsonic.github.io/summary/videos/grasp_DDPG.mp4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Language guided visual 3D indoor navigation</title>
      <link>https://davidsonic.github.io/academic_blog/post/3d-indoor/</link>
      <pubDate>Sat, 03 Nov 2018 14:47:53 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/post/3d-indoor/</guid>
      <description>&lt;p&gt;Poster for this research is available at &lt;a href=&#34;https://davidsonic.github.io/summary/Poster_3d_indoor.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;News about the project is &lt;a href=&#34;http://mcl.usc.edu/news/2018/12/02/mcl-research-on-re-enforcement-learning/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trajectory visualization is at &lt;a href=&#34;http://mcl-lab.usc.edu:3000/trajectory.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Brief report for this project is &lt;a href=&#34;https://davidsonic.github.io/summary/EE546Final-JialiDuan.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RL 3D Indoor Navigation</title>
      <link>https://davidsonic.github.io/academic_blog/project/rl-indoor-navigation/</link>
      <pubDate>Wed, 24 Oct 2018 17:03:21 +0800</pubDate>
      
      <guid>https://davidsonic.github.io/academic_blog/project/rl-indoor-navigation/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
